Comparador de Respostas de Modelos de LLMs

Este projeto realiza a comparaÃ§Ã£o de respostas geradas por diferentes modelos de Large Language Models (LLMs), incluindo Gemini, Mistral, Ollama (Mistral) e DeepSeek. A anÃ¡lise considera critÃ©rios como clareza, coerÃªncia, precisÃ£o da informaÃ§Ã£o e criatividade.
PrÃ©-requisitos

Antes de executar o projeto, certifique-se de que possui os seguintes requisitos instalados:

Python 3.10+

pip (gerenciador de pacotes do Python)

Ollama instalado e rodando localmente ou acessÃ­vel remotamente

InstalaÃ§Ã£o do Ollama

O Ollama precisa estar instalado na mÃ¡quina local para que este projeto funcione corretamente. Para instalÃ¡-lo, siga as instruÃ§Ãµes abaixo conforme o seu sistema operacional:

Windows:

winget install Ollama.Ollama

macOS:

brew install ollama

Linux:

curl -fsSL https://ollama.com/install.sh | sh

Executando o Ollama

ApÃ³s a instalaÃ§Ã£o, inicie o Ollama executando o seguinte comando:

ollama serve &

Isso garantirÃ¡ que ele esteja rodando localmente na porta 11434.

Caso prefira rodar o Ollama via Docker:
docker run -d -p 11434:11434 ollama/ollama

ğŸš€ Funcionalidades

ObtÃ©m respostas de mÃºltiplos modelos de LLMs.

Analisa as respostas com base em critÃ©rios linguÃ­sticos e semÃ¢nticos.

Normaliza as pontuaÃ§Ãµes entre 0 e 10.

Gera um ranking das respostas utilizando o Gemini.

ğŸ› ï¸ Tecnologias Utilizadas

Python

Google Gemini API

Mistral API

DeepSeek API

Ollama (Local LLM Serving)

LanguageTool (CorreÃ§Ã£o gramatical)

NumPy (NormalizaÃ§Ã£o de dados)

ğŸ“ pythonProject
â”‚-- config.py            # ConfiguraÃ§Ã£o das chaves de API
â”‚-- llm_compare.py       # FunÃ§Ãµes para obter respostas e analisÃ¡-las
â”‚-- rank_compare.py      # Ranking das respostas com o Gemini
â”‚-- main.py              # Arquivo principal para execuÃ§Ã£o
â”‚-- .env                 # Arquivo para armazenar as chaves de API

ğŸ”§ ConfiguraÃ§Ã£o

1ï¸âƒ£ Clonar o repositÃ³rio

git clone git@github.com:karolyne04/desafio-cognitiva.git


2ï¸âƒ£ Criar um ambiente virtual

python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows

3ï¸âƒ£ Instalar dependÃªncias

pip install -r requirements.txt

4ï¸âƒ£ Criar o arquivo .env e adicionar suas chaves de API


GOOGLE_API_KEY=AIzaSyCRI5wnhj1gqy3O2wg3wE_uEaRNpoSHAko
MISTRAL_API_KEY=N8qK57AYYdyUlfgRjF7Gr1VFOSRhWb7g
DEEPSEEK_API_KEY=sk-a7a3354a521c47a1bc786d6f73673d51



5ï¸âƒ£ Executar o projeto

python main.py

ğŸ“Š Como Funciona

ğŸ”¹ Coleta de Respostas

Cada modelo recebe a mesma pergunta e gera sua resposta:

responses = {
    "Gemini": get_gemini_response(question),
    "Mistral": get_mistral_response(question),
    "Ollama (Mistral)": get_ollama_response(question),
    "DeepSeek": get_deepseek_response(question),
}

ğŸ”¹ AnÃ¡lise das Respostas

A funÃ§Ã£o analyze_responses() avalia as respostas com base em:

Clareza e coerÃªncia (frases bem estruturadas)

PrecisÃ£o (comparaÃ§Ã£o com um texto de referÃªncia)

Criatividade e profundidade (presenÃ§a de exemplos e argumentos)

ConsistÃªncia gramatical (uso do LanguageTool)

ğŸ”¹ NormalizaÃ§Ã£o das Notas

Os escores sÃ£o normalizados para uma escala de 0 a 10.

results = normalize_scores(results)

ğŸ”¹ Ranking das Respostas

O Gemini classifica as respostas com base nos critÃ©rios acima.

ranking_result = rank_responses_with_gemini(responses)

ğŸ“œ Exemplo de SaÃ­da

ğŸ”¹ Gemini:
A InteligÃªncia Artificial Ã©...

ğŸ”¹ Mistral:
A IA pode ser definida como...

ğŸ”¹ Ollama (Mistral):
InteligÃªncia Artificial Ã©...

ğŸ”¹ DeepSeek:
A IA Ã© uma Ã¡rea da ciÃªncia da computaÃ§Ã£o...

ğŸ† Ranking das respostas:
1. Gemini - Melhor clareza e precisÃ£o
2. DeepSeek - Boa profundidade e criatividade
3. Mistral - Resposta coerente, mas genÃ©rica
4. Ollama - Resposta mais curta e superficial

 Desenvolvido por Carolyne
