import google.generativeai as genai
from mistralai.client import MistralClient
import requests
from config import google_key, mistral_key, deepseek_key
import re
import language_tool_python
import numpy as np


# üîπ Configura√ß√£o da API do Gemini
genai.configure(api_key=google_key)

# üîç Fun√ß√£o para obter resposta do Gemini
def get_gemini_response(question: str) -> str:
    try:
        model = genai.GenerativeModel("gemini-pro")
        response = model.generate_content(question)
        return response.text if response else "Erro ao obter resposta do Gemini."
    except Exception as e:
        return f"Erro Gemini: {str(e)}"

# üîç Fun√ß√£o para obter resposta do Mistral
def get_mistral_response(question: str) -> str:
    try:
        client = MistralClient(api_key=mistral_key)
        response = client.chat(
            model="mistral-medium",
            messages=[{"role": "user", "content": question}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Erro Mistral: {str(e)}"

# üîç Fun√ß√£o para obter resposta do Ollama
def get_ollama_response(question: str) -> str:
    try:
        url = "http://127.0.0.1:11434/api/generate"
        data = {"model": "mistral", "prompt": question, "stream": False}
        response = requests.post(url, json=data)
        if response.status_code == 200:
            return response.json().get("response", "Erro: Resposta vazia.")
        return f"Erro: {response.text}"
    except requests.exceptions.ConnectionError:
        return "Erro Ollama: Servidor n√£o est√° rodando. Execute 'ollama run mistral'."
    except Exception as e:
        return f"Erro Ollama: {str(e)}"

# üîç Fun√ß√£o para obter resposta do DeepSeek
def get_deepseek_response(question: str) -> str:
    try:
        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {deepseek_key}", "Content-Type": "application/json"}
        data = {"model": "deepseek-chat", "messages": [{"role": "user", "content": question}], "temperature": 0.7}

        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        elif response.status_code == 401:
            return "Erro DeepSeek: Chave de API inv√°lida ou sem permiss√£o."
        elif response.status_code == 402:
            return "Erro DeepSeek: Saldo insuficiente. Verifique sua conta."
        return f"Erro DeepSeek: {response.text}"
    except Exception as e:
        return f"Erro DeepSeek: {str(e)}"

# üîç Texto de refer√™ncia para compara√ß√£o
REFERENCE_TEXT = """
Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que visa criar sistemas capazes de simular processos de pensamento humano, como aprendizado, racioc√≠nio e resolu√ß√£o de problemas. 
Algumas abordagens incluem redes neurais, aprendizado profundo e algoritmos gen√©ticos. A IA √© aplicada em diversas √°reas, como sa√∫de, finan√ßas e tecnologia.
"""

# üìä Fun√ß√£o para calcular similaridade entre resposta e texto de refer√™ncia
def calculate_similarity(response: str, reference: str) -> float:
    response_words = set(response.lower().split())
    reference_words = set(reference.lower().split())
    intersection = len(response_words & reference_words)
    union = len(reference_words)
    return intersection / union if union > 0 else 0

# üìä Fun√ß√£o para analisar respostas dos modelos
def analyze_responses(responses):
    """
    Analisa as respostas dos modelos de IA com base em v√°rios crit√©rios:
    - Clareza e coer√™ncia (presen√ßa de senten√ßas bem estruturadas)
    - Precis√£o da informa√ß√£o (compara√ß√£o com o texto de refer√™ncia)
    - Criatividade e profundidade (quantidade de argumentos e exemplos)
    - Consist√™ncia gramatical (erros identificados pelo LanguageTool)
    """

    tool = language_tool_python.LanguageTool('pt-BR')

    results = {}

    for model, response in responses.items():
        # 1Ô∏è‚É£ Clareza e coer√™ncia: Contar frases bem estruturadas (m√≠nimo 5 palavras)
        sentences = re.split(r'[.!?]', response)
        well_structured_sentences = sum(1 for s in sentences if len(s.split()) >= 5)

        # 2Ô∏è‚É£ Precis√£o da informa√ß√£o: Compara√ß√£o com o texto de refer√™ncia
        precision_score = calculate_similarity(response, REFERENCE_TEXT)

        # 3Ô∏è‚É£ Criatividade e profundidade: Contar n√∫mero de argumentos/exemplos
        num_arguments = response.lower().count("por exemplo") + response.lower().count("como") + response.lower().count(
            "isto significa")

        # 4Ô∏è‚É£ Consist√™ncia gramatical: Verificar erros gramaticais
        grammar_errors = len(tool.check(response))

        # Armazena os resultados antes da normaliza√ß√£o
        results[model] = {
            "clareza_coerencia": well_structured_sentences,
            "precisao": precision_score,
            "criatividade_profundidade": num_arguments,
            "gramatica": -grammar_errors  # Quanto menos erros, melhor o score
        }

    # üî¢ Normaliza os scores para uma escala de 0 a 10
    results = normalize_scores(results)

    return results

# üî¢ Fun√ß√£o para normalizar os scores entre 0 e 10
def normalize_scores(results):
    criteria = ["clareza_coerencia", "precisao", "criatividade_profundidade", "gramatica"]

    for criterion in criteria:
        scores = [model_scores[criterion] for model_scores in results.values()]
        min_score, max_score = min(scores), max(scores)

        # Evita divis√£o por zero se todos os valores forem iguais
        if min_score == max_score:
            normalized_scores = [5] * len(scores)  # Se todos forem iguais, atribui um valor m√©dio
        else:
            normalized_scores = [((score - min_score) / (max_score - min_score)) * 10 for score in scores]

        # Atribui os valores normalizados de volta ao dicion√°rio
        for i, model in enumerate(results.keys()):
            results[model][criterion] = round(normalized_scores[i], 2)

    return results
